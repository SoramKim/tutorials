{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of LanguageModel.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_rmWcSt5yVD",
        "colab_type": "text"
      },
      "source": [
        "**Example of PennTreeBank Language Modeling**\n",
        "\n",
        "make sure that you have two directories\n",
        "\n",
        "*   drive/My Drive/public/data/ptb\n",
        "*   drive/My Drive/public/results\n",
        "\n",
        "make sure that you have three dataset files in 'drive/My Drive/public/data/ptb/' directory\n",
        "\n",
        "*   ptb.train.txt\n",
        "*   ptb.train.txt.pkl\n",
        "*   ptb.valid.txt\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pyQaleE_5nYk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I5WpTM3o89aR",
        "colab_type": "text"
      },
      "source": [
        "import pytorch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AN4q5hXC04t-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4f05aec0-5602-46e3-eec0-bf32f370a0c7"
      },
      "source": [
        "### Import the libraries\n",
        "\n",
        "import os\n",
        "import time\n",
        "import math\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import six; from six.moves import cPickle as pkl\n",
        "import numpy as np\n",
        "\n",
        "print(\"Importing libraries done!\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Importing libraries done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-3ymLZ79RiU",
        "colab_type": "text"
      },
      "source": [
        "define special tokens and util functinos\n",
        "\n",
        "\n",
        "*   ids2words : transform the tokenized sentence to real language sentence\n",
        "*   timeSince : compute the hours, minutes and seconds\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QFsgnchGI0Kb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BOS_token = 0 # Beginning Of Sentence token\n",
        "EOS_token = 1 # End Of Sentence token\n",
        "UNK_token = 2 # UNKnown token\n",
        "\n",
        "### Define the util functions\n",
        "def ids2words(dict_map, raw_data, sep=' ', eos_id=0, unk_sym='<unk>'):\n",
        "    str_text = ''\n",
        "    raw_data = raw_data.squeeze().tolist()\n",
        "\n",
        "    # Make the dict to inverse for translate unique number to word\n",
        "    dict_map_inv = dict()\n",
        "    for kk, vv in dict_map.items():\n",
        "        dict_map_inv[vv] = kk\n",
        " \n",
        "    for vv in raw_data:\n",
        "        if vv == eos_id:\n",
        "            break\n",
        "        if vv in dict_map_inv:\n",
        "            str_text = str_text + sep + dict_map_inv[vv]\n",
        "        else:\n",
        "            str_text = str_text + sep + unk_sym\n",
        "    return str_text.strip()\n",
        "\n",
        "def timeSince(since):\n",
        "  now = time.time()\n",
        "  s = now - since\n",
        "\n",
        "  h = math.floor(s / 3600)\n",
        "  m = math.floor((s-3600*h) / 60)\n",
        "  s = s - h*3600 - m*60\n",
        "\n",
        "  return '{}h {}m {:.3f}s'.format(h,m,s)\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p2HaqYVu92Pi",
        "colab_type": "text"
      },
      "source": [
        "define your Language Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gaMyddmDLZ3g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Make my Language Model\n",
        "class LM(nn.Module):\n",
        "    def __init__(self, dict_len, dim_enc, dim_wemb, device):\n",
        "        super(LM, self).__init__()\n",
        "        self.dim_enc = dim_enc\n",
        "        self.wemb = dim_wemb\n",
        "        self.dict_len = dict_len\n",
        "        self.device = device\n",
        "\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        self.src_emb = nn.Embedding(dict_len, dim_wemb)\n",
        "        self.rnn_enc = nn.LSTMCell(dim_wemb, dim_enc)\n",
        "\n",
        "        self.readout = nn.Linear(dim_enc, dim_wemb)\n",
        "        self.dec = nn.Linear(dim_wemb, dict_len)\n",
        "\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, data, mask=None): \n",
        "        # data : (Timeseq, Batch)\n",
        "        x_data = data[:-1] # input (the last word is not the input)\n",
        "        y_data = data[1:]  # label (the first word is not the label)\n",
        "        if mask is not None:\n",
        "            x_mask = mask[1:]\n",
        "            y_mask = mask[1:]\n",
        "\n",
        "        Tx, Bn = x_data.size()\n",
        "       \n",
        "        x_emb = self.src_emb(torch.reshape(x_data, (Bn*Tx,1)))\n",
        "        x_emb = x_emb.view(Tx,Bn,-1)\n",
        "        x_emb = self.dropout(x_emb)\n",
        "        # x_emb : (Timeseq, Batch, dim_wemb)\n",
        "        \n",
        "        ht = torch.zeros(Bn,self.dim_enc)\n",
        "        ct = torch.zeros(Bn,self.dim_enc)\n",
        "        ht = Variable(ht).to(self.device)\n",
        "        ct = Variable(ct).to(self.device)\n",
        "        \n",
        "        gen_sentence = x_data[0].unsqueeze(1) # (Batch, 1)\n",
        "        loss = 0\n",
        "        for i in range(Tx):\n",
        "            ht, ct = self.rnn_enc(x_emb[i,:,:],(ht, ct))\n",
        "            # ht, ct : (Batch, dim_enc)\n",
        "            output = self.readout(ht)\n",
        "            output = self.dropout(output)\n",
        "            # output : (Batch, dim_wemb)\n",
        "            logit = self.dec(output)\n",
        "            # logit : (Batch, dict_len)\n",
        "            loss_tmp = self.criterion(logit, y_data[i])\n",
        "            \n",
        "            probs = self.softmax(logit)\n",
        "            topv, yt = probs.topk(1) # Choose top 1 prob. word\n",
        "           \n",
        "            gen_sentence = torch.cat((gen_sentence, yt), dim=1)\n",
        "            if mask is not None:\n",
        "                loss += torch.sum(loss_tmp*y_mask[i])/Bn\n",
        "            else:\n",
        "                loss += torch.sum(loss_tmp)/Bn\n",
        "        \n",
        "        return loss, gen_sentence\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TfiPrNQONsgW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, device, train_loader, optimizer, epoch, log_interval):\n",
        "    model.train()\n",
        "    \n",
        "    loss_total = 0\n",
        "    den = 0\n",
        "    for batch_idx, (data, mask) in enumerate(train_loader):\n",
        "        data, mask = torch.transpose(data,1,0).to(device), torch.transpose(mask,1,0).to(device)\n",
        "        optimizer.zero_grad()\n",
        "        loss, gen_sentence = model(data, mask)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        " \n",
        "    print('Train Epoch: {} \\tLoss: {:.6f}'.format(\n",
        "                epoch, loss.item()))\n",
        "    real_sen = ids2words(src_dict, data[:,0], eos_id=EOS_token)\n",
        "    gen_sen = ids2words(src_dict, gen_sentence[0], eos_id=EOS_token)\n",
        "    print(\"train real sentence: {}\".format(real_sen))\n",
        "    print(\"train gen. sentence: {}\".format(gen_sen))\n",
        "    print(\"========================================\")        \n",
        "    return loss.item(), gen_sentence\n",
        "            \n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (data, mask) in enumerate(test_loader):\n",
        "            data, mask = torch.transpose(data,1,0).to(device), torch.transpose(mask,1,0).to(device)\n",
        "            data, mask = data.to(device), mask.to(device)\n",
        "            loss, gen_sentence = model(data)\n",
        "            test_loss += loss\n",
        "\n",
        "    test_loss /= batch_idx\n",
        "\n",
        "    print('\\nTest: Average loss: {:.4f}\\n'.format(\n",
        "        test_loss))\n",
        "    return test_loss\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p2AVB4B3LwqR",
        "colab_type": "text"
      },
      "source": [
        "define your own custom dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TQghL1vv1RDz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Make my custom Dataset and DatasetLoader classes\n",
        "class ptb_dataset(Dataset):\n",
        "    def __init__(self,train_data,data_dict,maxlen=30):\n",
        "        # Load the dataset and word_dict\n",
        "        self.train_data_raw = open(train_data, 'r')\n",
        "        with open(data_dict, 'rb') as f:\n",
        "            self.data_dict = pkl.load(f)\n",
        "\n",
        "        # Make dict has unique index\n",
        "        self.data_dict2 = dict()\n",
        "        for kk, vv in self.data_dict.items():\n",
        "            self.data_dict2[kk] = vv + 1\n",
        "        self.data_dict2['<s>'] = BOS_token\n",
        "\n",
        "        self.maxlen = maxlen\n",
        "        \n",
        "        # Pre-processing the datasets\n",
        "        self.train_data, self.train_len = self.data_init(self.train_data_raw)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        sentence = self.train_data[index,:self.train_len[index]]\n",
        "        x_data, x_mask = self.prepare_text(sentence)\n",
        "        return torch.tensor(x_data).type(torch.long),\\\n",
        "               torch.tensor(x_mask).type(torch.float)\n",
        " \n",
        "    def __len__(self):\n",
        "        return len(self.train_data)\n",
        "\n",
        "    def dict_len(self):\n",
        "        return len(self.data_dict2)\n",
        "    \n",
        "    def use_dict(self):\n",
        "        return self.data_dict2\n",
        "\n",
        "    def data_init(self, data):\n",
        "        #Check the number of sample < maxlen\n",
        "        num = 0\n",
        "        while True:\n",
        "            sentence = data.readline()\n",
        "            if sentence == \"\":\n",
        "                break\n",
        "            if len(sentence.strip().split()) >= self.maxlen:\n",
        "                continue\n",
        "            else:\n",
        "                num += 1    \n",
        "        # Make the preprocessed dataset\n",
        "        dataset = np.zeros((num, self.maxlen))\n",
        "        data_len = np.zeros(num, dtype=np.int)\n",
        "        idx = 0\n",
        "        data.seek(0)\n",
        "        while True:\n",
        "            sentence = data.readline()\n",
        "            if sentence == \"\": # End of the dataset\n",
        "                break\n",
        "            # Make sentence to word level (splitted by space)\n",
        "            sentence = sentence.strip().split() \n",
        "            if len(sentence) >= self.maxlen:\n",
        "                continue\n",
        "            else:\n",
        "                sentence = [self.data_dict2.get(key, UNK_token)\\\n",
        "                                         for key in sentence]\n",
        "                dataset[idx,:len(sentence)] = sentence\n",
        "                data_len[idx] = len(sentence)\n",
        "                idx += 1\n",
        "        return dataset, data_len\n",
        "    \n",
        "    def prepare_text(self, sentence):\n",
        "        maxlen = self.maxlen + 2 # +2 for BOS and EOS\n",
        "        x_data = np.ones(maxlen).astype('int64')\n",
        "        x_mask = np.zeros(maxlen).astype('float32')\n",
        "        x_data[1:len(sentence)+1] = sentence\n",
        "        x_data[0] = BOS_token\n",
        "        x_mask[:len(sentence)+2] = 1. # EOS token\n",
        "\n",
        "        return x_data, x_mask    \n",
        "\n",
        "def ptb_loader(train_data, batch_size, maxlen):\n",
        "    data_dict = 'drive/My Drive/public/data/ptb/ptb.train.txt.pkl'\n",
        "\n",
        "    data = ptb_dataset(train_data, data_dict, maxlen=maxlen)\n",
        "    src_dict = data.use_dict()\n",
        "    \n",
        "    data_loader = DataLoader(data, batch_size=batch_size, shuffle=False)\n",
        "    \n",
        "    return data_loader, data.dict_len(), src_dict\n",
        "\n",
        "  "
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vziddcLM_Wbu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Test my dataset, datasetloader classes\n",
        "batch_size = 1\n",
        "maxlen = 30\n",
        "\n",
        "train_data = 'drive/My Drive/public/data/ptb/ptb.train.txt'\n",
        "train_loader, dict_len, src_dict = ptb_loader(train_data, batch_size, maxlen)\n",
        "\n",
        "for i, (x_data, x_mask) in enumerate(train_loader):\n",
        "    real_sen = ids2words(src_dict, x_data, eos_id=EOS_token)\n",
        "    print(\"----------------------------------------------------------\")\n",
        "    print(\"real sentence: \")\n",
        "    print(real_sen)\n",
        "    print(\"x_data.shape: \", x_data.shape)\n",
        "    print(\"x_data:\")\n",
        "    print(x_data)\n",
        "    print(\"x_mask:\")\n",
        "    print(x_mask)\n",
        "    print(\"----------------------------------------------------------\")\n",
        "\n",
        "    if i >= 5:\n",
        "        break\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Qb3mr1VoM21",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Hyperparameters\n",
        "batch_size = 64\n",
        "test_batch_size=1\n",
        "maxlen = 30\n",
        "dim_enc = 400\n",
        "dim_emb = 300\n",
        "lr = 0.0001\n",
        "optimizer = 'Adam'\n",
        "max_epoch = 100\n",
        "log_interval = 100"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v8ujSD18OK-u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Training\n",
        "train_data = 'drive/My Drive/public/data/ptb/ptb.train.txt'\n",
        "test_data = 'drive/My Drive/public/data/ptb/ptb.valid.txt'\n",
        "\n",
        "# Check the device\n",
        "cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
        "\n",
        "# build my dataset loader\n",
        "train_loader, dict_len, src_dict = ptb_loader(train_data, batch_size, maxlen)\n",
        "test_loader, _, _ = ptb_loader(test_data, test_batch_size, maxlen)\n",
        "\n",
        "# build my model\n",
        "model = LM(dict_len, dim_enc, dim_emb, device)\n",
        "model.to(device)\n",
        "\n",
        "# build the optimizer\n",
        "if optimizer == 'RMSprop':\n",
        "    opt = optim.RMSprop(model.parameters(), lr=lr)\n",
        "elif optimizer == 'Adam':\n",
        "    opt = optim.Adam(model.parameters(), lr=lr)\n",
        "elif optimizer == 'Adadelta':\n",
        "    opt = optim.Adadelta(model.parameters(), lr=lr)\n",
        "else:\n",
        "    opt = optim.SGD(model.parameters(), lr=lr)\n",
        "\n",
        "# Training..\n",
        "print(\"Training Start!\")\n",
        "best_loss = 99999\n",
        "for epoch in range(max_epoch):\n",
        "    train(model, device, train_loader, opt, epoch, log_interval)\n",
        "    test_loss = test(model, device, test_loader)\n",
        "\n",
        "    if best_loss > test_loss:\n",
        "        print(\"We found the best model!\")\n",
        "        best_loss = test_loss\n",
        "        save_dir = 'drive/My Drive/public/results/ptb_trained_model_best.pth'\n",
        "        if os.path.exists(save_dir):\n",
        "            os.remove(save_dir)\n",
        "        torch.save(model, save_dir)\n",
        "\n",
        "print(\"Training is done!!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUpFs90x7sxC",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    }
  ]
}